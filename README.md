# Проект по созданию DataMart
Данный проект направлен на создание DataMart, содержащей идентификаторы пользователей для определенного веб-сайта. DataMart строится на основе Витрины A, которая разделена по датам.

## Обзор
Проект выполняет следующие задачи:

Генерирует синтетические данные для различных таблиц (A, B, C, D, E, F, G), представляющих различные идентификаторы пользователей.
Реализует алгоритм создания DataMart путем объединения и удаления дубликатов идентификаторов пользователей из разных таблиц.
Поддерживает ежедневное выполнение для добавления новых данных и удаления дубликатов.

## Требования
Apache Spark (версия 3.1.2)
Python (версия 3.8.5)
Faker (Python-библиотека для генерации синтетических данных)

## Структура проекта
Проект имеет следующую структуру:

 - gen_data.py: Содержит код для генерации синтетических данных для различных таблиц.
 - create_dm.py: Реализует алгоритм создания DataMart.
 - config.py: Файл конфигурации для настроек Spark.
 - README.md: Этот файл, предоставляющий обзор проекта.

##Использование
1. Установите необходимые зависимости:

2. Copy code
    pip install faker
    Запустите скрипт генерации данных для создания синтетических данных для таблиц:

3.  Copy code
    python gen_data.py
    Запустите скрипт создания DataMart для создания DataMart:

4.  Copy code
    python create_dm.py
    В результате будет создана DataMart, содержащая объединенные и удаленные дубликаты идентификаторов пользователей.

## Конфигурация
Настройки Spark хранятся в файле config.py. Вы можете изменить настройки Spark в этом файле в соответствии с вашими требованиями.

spark.sql.shuffle.partitions: Это количество партиций, используемых для операций перемешивания данных, таких как объединение и сортировка. Значение 5 выбрано для демонстрационных целей, но в реальном проекте его следует выбирать исходя из доступных ресурсов и объема данных. Большее число партиций может увеличить параллелизм, но требует больше ресурсов.

spark.executor.memory и spark.driver.memory: Это объем памяти, выделенной для каждого исполнителя Spark (executor) и для драйвера Spark (driver). Значение 2g выбрано в примере для демонстрационных целей. Однако, в реальном проекте вам следует выбрать соответствующие значения, учитывая объем доступной памяти на ваших узлах и требования вашей задачи.


## Ответ на вопрос: Как и когда имеет смысл использовать "spark.sql.autoBroadcastJoinThreshold" ? какие параметры можно передавать в этот конфиг?

Параметр "spark.sql.autoBroadcastJoinThreshold" в Spark позволяет автоматически определить размер таблицы для выполнения широковещательного соединения (broadcast join). Широковещательное соединение происходит, когда маленькая таблица отправляется на все узлы кластера, чтобы избежать перемещения большой таблицы между узлами.

Значение этого параметра представляет собой размер таблицы, который должен быть ниже указанного порога, чтобы Spark считал его маленьким и использовал широковещательное соединение. По умолчанию значение параметра равно 10 MB.

Использование широковещательного соединения имеет смысл в следующих случаях:

Когда одна из таблиц является очень маленькой по сравнению с другой таблицей, и ее можно безопасно отправить на все узлы кластера.
Когда перемещение большой таблицы между узлами занимает значительное время и ухудшает производительность.
Параметры, которые можно передать в конфигурацию "spark.sql.autoBroadcastJoinThreshold":

Размер в байтах: Можно указать конкретный размер в байтах, например, "10000000" для 10 MB.
"10m", "16g", и т.д.: Можно использовать суффиксы "k" (килобайты), "m" (мегабайты), "g" (гигабайты) и т.д. для указания размера.

Примеры:

spark.sql.autoBroadcastJoinThreshold = 10485760 (10 MB)
spark.sql.autoBroadcastJoinThreshold = "16m" (16 мегабайт)
Важно учитывать размер доступной памяти в кластере и оценивать размер таблиц перед принятием решения об использовании широковещательного соединения.

